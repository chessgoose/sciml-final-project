{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57334e36-39bf-4342-ab23-5bf0868237b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ab4de19-cc35-47c2-912c-d1dc3372c39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys, os, json\n",
    "import mne, sklearn, wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "from nilearn import datasets, image, masking, plotting\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "\n",
    "# animation part\n",
    "from IPython.display import HTML\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# from celluloid import Camera   # it is convinient method to animate\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "## torch libraries \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "from utils import get_datasets\n",
    "from utils import preproc\n",
    "from utils import torch_dataset\n",
    "from utils import train_utils\n",
    "from utils import inference\n",
    "from utils.models_arch import autoencoder_new_Artur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68eacc99-823f-49be-80b1-b432a1833d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0974b520-53af-4aae-8475-ad9864cc86f0",
   "metadata": {},
   "source": [
    "# Set all hyperparameters\n",
    "- Cuda and GPU.\n",
    "- Parameters of dataset. \n",
    "- random seed( if necessary). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9740a7b-6c36-471f-bafe-3361c4758086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 1\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "# random.seed(0)  # python operation seed\n",
    "# np.random.seed(0)\n",
    "\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(torch.cuda.is_available(), torch.cuda.device_count())\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e68da9ec-be91-4cc8-b7d2-87c106d53ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(  \n",
    "                dataset_name = 'CWL_raw', # CWL\n",
    "                patients = ['trio1'], # work only with one patient\n",
    "                fps = 1000,\n",
    "                new_fps = 100, \n",
    "                crop_start = 5,\n",
    "                freqs = [-1], \n",
    "    \n",
    "                n_channels = 30, # 63 \n",
    "                n_roi = 8,\n",
    "                \n",
    "                bold_delay = 6,\n",
    "                to_many = True,\n",
    "                random_subsample = True,\n",
    "                sample_per_epoch = 512, \n",
    "                WINDOW_SIZE = 2048,\n",
    "                    \n",
    "                optimizer='adamW',\n",
    "                lr=1e-5,  # 5e-5 is too big LMAO\n",
    "                weight_decay=1e-4, \n",
    "                batch_size=16, \n",
    "                    \n",
    "                mse_weight = 1.0,\n",
    "                corr_weight = 0.0,\n",
    "                \n",
    "                preproc_type = 'dB_log',\n",
    "                loss_function = 'corr', \n",
    "                model_type = 'Best_AE_Artur_Multi_Head'\n",
    "                )\n",
    "\n",
    "\n",
    "hp_autoencoder = dict(n_electrodes=config['n_channels'],\n",
    "                      n_freqs = len(config['freqs']),\n",
    "                      n_channels_out = config['n_roi'],\n",
    "\n",
    "                     channels = [128, 128, 128, 128], \n",
    "                     kernel_sizes=[5, 5, 3],\n",
    "                     strides=[8, 8, 4], \n",
    "                     dilation=[1, 1, 1], \n",
    "                     decoder_reduce=4, \n",
    "                     hidden_channels = 16,\n",
    "                     )\n",
    "\n",
    "\n",
    "config = {**hp_autoencoder, **config}\n",
    "\n",
    "params_train = {'batch_size': config['batch_size'],\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0}\n",
    "\n",
    "params_val = {'batch_size': config['batch_size'],\n",
    "              'shuffle': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a1c42-68f1-4afa-9943-02a441b15e07",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Upload preprocessed dataset from np files. \n",
    "It should accelerate speed of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9caa3d86-8e41-4b0e-b624-4cf0b264bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../data/interim/labels_roi_17.json\", 'r') as f:\n",
    "#     labels_roi_17 = json.load(f)\n",
    "    \n",
    "labels_roi = ['Left Pallidum',\n",
    "                 'Left Caudate',\n",
    "                 'Left Putamen',\n",
    "                 'Left Accumbens',\n",
    "\n",
    "                 'Right Pallidum',\n",
    "                 'Right Caudate',\n",
    "                 'Right Putamen',\n",
    "                 'Right Accumbens']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e502a9-8cdb-48ca-86b8-0525448cb7c3",
   "metadata": {},
   "source": [
    "## Download CWL dataset\n",
    "\n",
    "You can save it into numpy files and the train only on such files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93b967f4-88c1-4b53-ad29-44a97761b112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL path:  ../data/eyes_open_closed_dataset/trio1/CWL_Data/eeg/in-scan/trio1_mrcorrected_eoec_in-scan_hpump-off.set ../data/eyes_open_closed_dataset/trio1/CWL_Data/mri/epi_normalized/rwatrio1_eoec_in-scan_hpump-off.nii ../data/eyes_open_closed_dataset/trio1/CWL_Data/mri/epi_motionparams/rp_atrio1_eoec_in-scan_hpump-off.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/common/koval_alvi/eeg-fmri-project/utils/get_datasets.py:215: RuntimeWarning: Data will be preloaded. preload=False or a string preload is not supported when the data is stored in the .set file\n",
      "  raw = mne.io.read_raw_eeglab(eeg_path_set_file)\n",
      "/home/user/common/koval_alvi/eeg-fmri-project/utils/get_datasets.py:215: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(eeg_path_set_file)\n",
      "/home/user/common/koval_alvi/eeg-fmri-project/utils/get_datasets.py:173: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  motion_confound = pd.read_csv(motion_params_path, sep = '  ', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of our EEG data:  (303601, 31)\n",
      "Dimension of our fMRi data:  (61, 72, 61, 146)\n",
      "Dimension of our fMRi Roi data:  (143, 18)\n",
      "fMRI info :  1.95\n",
      "RoI:  ['Left Lateral Ventricle', 'Left Thalamus', 'Left Caudate', 'Left Putamen', 'Left Pallidum', 'Brain-Stem', 'Left Hippocampus', 'Left Amygdala', 'Left Accumbens', 'Right Lateral Ventricle', 'Right Thalamus', 'Right Caudate', 'Right Putamen', 'Right Pallidum', 'Right Hippocampus', 'Right Amygdala', 'Right Accumbens', 'time']\n",
      "Original FPS 1000\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 6601 samples (6.601 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/common/koval_alvi/eeg-fmri-project/utils/get_datasets.py:261: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  return df_eeg, df_fmri, df_fmri.drop(['time'], 1).columns.to_list()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up band-pass filter from 1 - 1e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 100.00 Hz\n",
      "- Upper transition bandwidth: 25.00 Hz (-6 dB cutoff frequency: 112.50 Hz)\n",
      "- Filter length: 3301 samples (3.301 sec)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_dataset = '../data/eyes_open_closed_dataset/'\n",
    "dataset_name = 'CWL'\n",
    "remove_confounds = True\n",
    "\n",
    "for patient in config['patients']:\n",
    "    df_eeg_cwl_raw, df_fmri_cwl_raw, labels_roi_17 = get_datasets.download_cwl_dataset(patient, path_to_dataset, \n",
    "                                                                        remove_confounds=remove_confounds,\n",
    "                                                                        verbose=True)\n",
    "    df_eeg_cwl, df_fmri_cwl, fps = get_datasets.interpolate_df_eeg_fmri(df_eeg_cwl_raw, df_fmri_cwl_raw)\n",
    "    \n",
    "    \n",
    "    config['fps'] = fps\n",
    "    print('Original FPS', config['fps']) \n",
    "    \n",
    "    # delete time columns. \n",
    "    # reshape [time, ch] -> [ch, time]\n",
    "    eeg_np = df_eeg_cwl.drop(['time'], axis=1).to_numpy().T\n",
    "    fmri_np = df_fmri_cwl.drop(['time'], axis=1).to_numpy().T\n",
    "    \n",
    "    \n",
    "    eeg_np = preproc.low_level_preproc_eeg(eeg_np, fps)\n",
    "    \n",
    "    data = {'eeg': eeg_np, \n",
    "            'fmri': fmri_np}\n",
    "    \n",
    "    # np.savez(f'../data/preproc/{config[\"dataset_name\"]}/{patient}_{config[\"fps\"]}_filtered_data', \n",
    "    #          eeg=eeg_np, fmri=fmri_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a2b872-5560-4eda-9fff-499dfaa2d322",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eeg_cwl_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb Cell 11\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df_eeg_cwl_raw[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m], df_fmri_cwl_raw[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_eeg_cwl_raw' is not defined"
     ]
    }
   ],
   "source": [
    "df_eeg_cwl_raw['time'], df_fmri_cwl_raw['time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756fc8b6-9918-4d6d-b517-b6bb74b238c5",
   "metadata": {},
   "source": [
    "## Preprocessing of datasets\n",
    "\n",
    "- get only useful fmri ROI \n",
    "- Cut starting points \n",
    "- Normalize \n",
    "- Splitting on train/test\n",
    "- downsampling to new_fps \n",
    "- Shifting between EEG and FMRI \n",
    "- Dataset and DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951efaec-ba20-48c9-94aa-588f3f5b13e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb Cell 13\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m../labels_roi_17.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     labels_roi_17 \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m      <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Define the file path using config and patient variables\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../data/preproc/\u001b[39m\u001b[39m{\u001b[39;00mconfig[\u001b[39m'\u001b[39m\u001b[39mdataset_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mconfig[\u001b[39m'\u001b[39m\u001b[39mpatients\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mconfig[\u001b[39m'\u001b[39m\u001b[39mfps\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m_filtered_data.npz\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../labels_roi_17.json', 'r') as file:\n",
    "    labels_roi_17 = json.load(file)\n",
    "    \n",
    "# Define the file path using config and patient variables\n",
    "file_path = f\"../data/preproc/{config['dataset_name']}/{config['patients'][0]}_{config['fps']}_filtered_data.npz\"\n",
    "\n",
    "\n",
    "\n",
    "# Load the data from the .npz file\n",
    "data = np.load(file_path)\n",
    "\n",
    "eeg, fmri = data['eeg'], data['fmri']\n",
    "df = pd.DataFrame(data = fmri.T, columns=labels_roi_17)\n",
    "df_filter = df[labels_roi]\n",
    "fmri = df_filter.to_numpy().T\n",
    "\n",
    "# crop start\n",
    "train_crop = config['crop_start']*config['fps']\n",
    "eeg, fmri = eeg[..., train_crop:], fmri[..., train_crop:]\n",
    "\n",
    "# normalize \n",
    "eeg = eeg / np.std(eeg)\n",
    "fmri, fmri_means_stds = preproc.normalize_data(fmri)\n",
    "\n",
    "# train/test split\n",
    "test_time = int(60*config['fps'])\n",
    "train_dataset_prep = (eeg[..., :-test_time], fmri[..., :-test_time])\n",
    "test_dataset_prep = (eeg[..., -test_time:], fmri[..., -test_time:])\n",
    "\n",
    "\n",
    "ds_factor = config['fps']/config['new_fps']\n",
    "train_dataset_prep = preproc.downsample_dataset(train_dataset_prep, factor = ds_factor)\n",
    "test_dataset_prep = preproc.downsample_dataset(test_dataset_prep, factor = ds_factor)\n",
    "\n",
    "# apply time dealy corrected\n",
    "train_dataset_prep = preproc.bold_time_delay_align(train_dataset_prep, \n",
    "                                                   config['new_fps'],\n",
    "                                                   config['bold_delay'])\n",
    "test_dataset_prep = preproc.bold_time_delay_align(test_dataset_prep, \n",
    "                                                  config['new_fps'],\n",
    "                                                  config['bold_delay'])\n",
    "\n",
    "\n",
    "print('Size of train dataset:', train_dataset_prep[0].shape, train_dataset_prep[1].shape)\n",
    "print('Size of test dataset:', test_dataset_prep[0].shape, test_dataset_prep[1].shape)\n",
    "\n",
    "# torch dataset creation \n",
    "torch_dataset_train = torch_dataset.CreateDataset_eeg_fmri(train_dataset_prep, \n",
    "                                                            random_sample=config['random_subsample'], \n",
    "                                                            sample_per_epoch=config['sample_per_epoch'], \n",
    "                                                            to_many=config['to_many'], \n",
    "                                                            window_size = config['WINDOW_SIZE'])\n",
    "\n",
    "torch_dataset_test = torch_dataset.CreateDataset_eeg_fmri(test_dataset_prep, \n",
    "                                                            random_sample=False, \n",
    "                                                            sample_per_epoch=None, \n",
    "                                                            to_many=config['to_many'], \n",
    "                                                            window_size = config['WINDOW_SIZE'])\n",
    "print('Size of test dataset:', len(torch_dataset_test))\n",
    "# because you do not have strid for val data. \n",
    "torch_dataset_test = Subset(torch_dataset_test, np.arange(len(torch_dataset_test))[::100])\n",
    "\n",
    "# init dataloaders for training\n",
    "train_loader = torch.utils.data.DataLoader(torch_dataset_train, **params_train)\n",
    "val_loader = torch.utils.data.DataLoader(torch_dataset_test, **params_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff0863-786a-4c24-9130-c997833869c1",
   "metadata": {},
   "source": [
    "# Init Model, Loss, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdbf8773-4974-4ee4-bf7a-758c704f4192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "            Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "   AutoEncoder1D_Artur-1        [4, 1, 2048]         245,057         245,057\n",
      "   AutoEncoder1D_Artur-2        [4, 1, 2048]         245,057         245,057\n",
      "   AutoEncoder1D_Artur-3        [4, 1, 2048]         245,057         245,057\n",
      "   AutoEncoder1D_Artur-4        [4, 1, 2048]         245,057         245,057\n",
      "   AutoEncoder1D_Artur-5        [4, 1, 2048]         245,057         245,057\n",
      "   AutoEncoder1D_Artur-6        [4, 1, 2048]         245,057         245,057\n",
      "   AutoEncoder1D_Artur-7        [4, 1, 2048]         245,057         245,057\n",
      "   AutoEncoder1D_Artur-8        [4, 1, 2048]         245,057         245,057\n",
      "=============================================================================\n",
      "Total params: 1,960,456\n",
      "Trainable params: 1,960,456\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = autoencoder_new_Artur.AutoEncoder1D_Artur_MultiHead(hp_autoencoder)\n",
    "\n",
    "print(summary(model, torch.zeros(4, config['n_channels'],\n",
    "                                 config['WINDOW_SIZE']), show_input=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e4982-344f-4d44-8d7e-445f4c4a5662",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28d21727-423a-4872-8bec-82fbd3b4fb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/tmp.9tdT0GhyR8/ipykernel_3837830/1333472953.py 38 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb Cell 17\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m parameters \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mEPOCHS\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1500\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m: model, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mto_many\u001b[39m\u001b[39m'\u001b[39m: config[\u001b[39m'\u001b[39m\u001b[39mto_many\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m path_to_save_wandb \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/lyz6/Documents\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mwith\u001b[39;00m wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39meeg_fmri\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49mconfig, save_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     wandb\u001b[39m.\u001b[39mdefine_metric(\u001b[39m\"\u001b[39m\u001b[39mval/corr_mean\u001b[39m\u001b[39m\"\u001b[39m, summary\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ood-mccleary.ycrc.yale.edu/home/lyz6/finalproj/beira/notebook_clean/BEIRA_ALL_IN_ONE.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \n",
      "File \u001b[0;32m~/.conda/envs/myenv_torch/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:953\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[1;32m    952\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[0;32m--> 953\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    954\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    955\u001b[0m     error_seen \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m~/.conda/envs/myenv_torch/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:931\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m    929\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m    930\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m    932\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m    933\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/myenv_torch/lib/python3.9/site-packages/wandb/sdk/wandb_init.py:564\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mcommunicating current version\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 564\u001b[0m     check \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39;49minterface\u001b[39m.\u001b[39;49mcommunicate_check_version(\n\u001b[1;32m    565\u001b[0m         current_version\u001b[39m=\u001b[39;49mwandb\u001b[39m.\u001b[39;49m__version__\n\u001b[1;32m    566\u001b[0m     )\n\u001b[1;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m check:\n\u001b[1;32m    568\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mgot version response \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(check))\n",
      "File \u001b[0;32m~/.conda/envs/myenv_torch/lib/python3.9/site-packages/wandb/sdk/interface/interface.py:92\u001b[0m, in \u001b[0;36mInterfaceBase.communicate_check_version\u001b[0;34m(self, current_version)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m current_version:\n\u001b[1;32m     91\u001b[0m     check_version\u001b[39m.\u001b[39mcurrent_version \u001b[39m=\u001b[39m current_version\n\u001b[0;32m---> 92\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate_check_version(check_version)\n\u001b[1;32m     93\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.conda/envs/myenv_torch/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py:413\u001b[0m, in \u001b[0;36mInterfaceShared._communicate_check_version\u001b[0;34m(self, check_version)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_communicate_check_version\u001b[39m(\n\u001b[1;32m    410\u001b[0m     \u001b[39mself\u001b[39m, check_version: pb\u001b[39m.\u001b[39mCheckVersionRequest\n\u001b[1;32m    411\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[pb\u001b[39m.\u001b[39mCheckVersionResponse]:\n\u001b[1;32m    412\u001b[0m     rec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(check_version\u001b[39m=\u001b[39mcheck_version)\n\u001b[0;32m--> 413\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate(rec)\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m         \u001b[39m# Note: timeouts handled by callers: wandb_init.py\u001b[39;00m\n\u001b[1;32m    416\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/myenv_torch/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py:213\u001b[0m, in \u001b[0;36mInterfaceShared._communicate\u001b[0;34m(self, rec, timeout, local)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_communicate\u001b[39m(\n\u001b[1;32m    211\u001b[0m     \u001b[39mself\u001b[39m, rec: pb\u001b[39m.\u001b[39mRecord, timeout: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, local: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    212\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[pb\u001b[39m.\u001b[39mResult]:\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_communicate_async(rec, local\u001b[39m=\u001b[39;49mlocal)\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m~/.conda/envs/myenv_torch/lib/python3.9/site-packages/wandb/sdk/interface/router.py:37\u001b[0m, in \u001b[0;36mMessageFutureObject.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[\u001b[39m\"\u001b[39m\u001b[39mpb.Result\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m---> 37\u001b[0m     is_set \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object_ready\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m is_set \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object:\n\u001b[1;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\n",
      "File \u001b[0;32m~/.conda/envs/myenv_torch/lib/python3.9/threading.py:574\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 574\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.conda/envs/myenv_torch/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_runs = 1\n",
    "\n",
    "for i in range(n_runs):\n",
    "    \n",
    "    model = autoencoder_new_Artur.AutoEncoder1D_Artur_MultiHead(hp_autoencoder)\n",
    "\n",
    "    loss_func = train_utils.make_complex_loss_function(mse_weight = config['mse_weight'], \n",
    "                                                       corr_weight = config['corr_weight'],\n",
    "                                                       manifold_weight = 0,\n",
    "                                                       bound=1)\n",
    "    \n",
    "    train_step = train_utils.train_step\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=config['lr'], \n",
    "                       weight_decay=config['weight_decay'])\n",
    "    \n",
    "    \n",
    "    parameters = {\n",
    "        'EPOCHS': 1500,\n",
    "        'model': model, \n",
    "        'train_loader': train_loader, \n",
    "        'val_loader': val_loader, \n",
    "        'loss_function': loss_func,\n",
    "        'train_step': train_step,\n",
    "        'optimizer': optimizer, \n",
    "        'device': 'cuda', \n",
    "        'raw_test_data': test_dataset_prep,\n",
    "        'show_info': 30, \n",
    "        'num_losses': 5,\n",
    "        'labels': labels_roi,\n",
    "        'inference_function': inference.model_inference_function, \n",
    "        'to_many': config['to_many']\n",
    "    }\n",
    "\n",
    "    path_to_save_wandb = '/home/lyz6/Documents'\n",
    "\n",
    "    with wandb.init(project=\"eeg_fmri\", config=config, save_code=True):\n",
    "        wandb.define_metric(\"val/corr_mean\", summary=\"max\")\n",
    "\n",
    "        if i == 0: \n",
    "            exp_name = wandb.run.name\n",
    "        \n",
    "        wandb.run.name = exp_name +'_run_' + str(i)\n",
    "        \n",
    "        print(config)\n",
    "        print(parameters['model'])\n",
    "        print(summary(model, torch.zeros(4, config['n_channels'], config['WINDOW_SIZE']), show_input=False))\n",
    "        \n",
    "        model = train_utils.wanb_train_regression(**parameters)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a129625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: (30, 20590) (8, 20590)\n",
      "Size of test dataset: (30, 5400) (8, 5400)\n",
      "Size of test dataset: 3351\n"
     ]
    }
   ],
   "source": [
    "# DEEPONET\n",
    "import json\n",
    "\n",
    "with open('../labels_roi_17.json', 'r') as file:\n",
    "    labels_roi_17 = json.load(file)\n",
    "    \n",
    "# Define the file path using config and patient variables\n",
    "file_path = f\"../data/preproc/{config['dataset_name']}/{config['patients'][0]}_{config['fps']}_filtered_data.npz\"\n",
    "\n",
    "# Load the data from the .npz file\n",
    "data = np.load(file_path)\n",
    "\n",
    "eeg, fmri = data['eeg'], data['fmri']\n",
    "df = pd.DataFrame(data = fmri.T, columns=labels_roi_17)\n",
    "df_filter = df[labels_roi]\n",
    "fmri = df_filter.to_numpy().T\n",
    "\n",
    "# crop start\n",
    "train_crop = config['crop_start']*config['fps']\n",
    "eeg, fmri = eeg[..., train_crop:], fmri[..., train_crop:]\n",
    "\n",
    "# normalize \n",
    "eeg = eeg / np.std(eeg)\n",
    "fmri, fmri_means_stds = preproc.normalize_data(fmri)\n",
    "\n",
    "# train/test split\n",
    "test_time = int(60*config['fps'])\n",
    "train_dataset_prep = (eeg[..., :-test_time], fmri[..., :-test_time])\n",
    "test_dataset_prep = (eeg[..., -test_time:], fmri[..., -test_time:])\n",
    "\n",
    "\n",
    "ds_factor = config['fps']/config['new_fps']\n",
    "train_dataset_prep = preproc.downsample_dataset(train_dataset_prep, factor = ds_factor)\n",
    "test_dataset_prep = preproc.downsample_dataset(test_dataset_prep, factor = ds_factor)\n",
    "\n",
    "# apply time dealy corrected\n",
    "train_dataset_prep = preproc.bold_time_delay_align(train_dataset_prep, \n",
    "                                                   config['new_fps'],\n",
    "                                                   config['bold_delay'])\n",
    "test_dataset_prep = preproc.bold_time_delay_align(test_dataset_prep, \n",
    "                                                  config['new_fps'],\n",
    "                                                  config['bold_delay'])\n",
    "\n",
    "\n",
    "print('Size of train dataset:', train_dataset_prep[0].shape, train_dataset_prep[1].shape)\n",
    "print('Size of test dataset:', test_dataset_prep[0].shape, test_dataset_prep[1].shape)\n",
    "\n",
    "# torch dataset creation \n",
    "torch_dataset_train = torch_dataset.CreateDeepONetDataset(train_dataset_prep, \n",
    "                                                            random_sample=config['random_subsample'], \n",
    "                                                            sample_per_epoch=config['sample_per_epoch'], \n",
    "                                                            to_many=config['to_many'], \n",
    "                                                            window_size = config['WINDOW_SIZE'])\n",
    "\n",
    "torch_dataset_test = torch_dataset.CreateDeepONetDataset(test_dataset_prep, \n",
    "                                                            random_sample=False, \n",
    "                                                            sample_per_epoch=None, \n",
    "                                                            to_many=config['to_many'], \n",
    "                                                            window_size = config['WINDOW_SIZE'])\n",
    "print('Size of test dataset:', len(torch_dataset_test))\n",
    "# because you do not have strid for val data. \n",
    "torch_dataset_test = Subset(torch_dataset_test, np.arange(len(torch_dataset_test))[::100])\n",
    "\n",
    "# init dataloaders for training\n",
    "train_loader = torch.utils.data.DataLoader(torch_dataset_train, **params_train)\n",
    "val_loader = torch.utils.data.DataLoader(torch_dataset_test, **params_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c923ea3f-b6c9-495e-bea2-36552a50cb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.18.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/lawrence-z/eeg_fmri/runs/38a7w9rn\" target=\"_blank\">sleek-paper-134</a></strong> to <a href=\"https://wandb.ai/lawrence-z/eeg_fmri\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_electrodes': 30, 'n_freqs': 1, 'n_channels_out': 8, 'channels': [128, 128, 128, 128], 'kernel_sizes': [5, 5, 3], 'strides': [8, 8, 4], 'dilation': [1, 1, 1], 'decoder_reduce': 4, 'hidden_channels': 16, 'dataset_name': 'CWL_raw', 'patients': ['trio1'], 'fps': 1000, 'new_fps': 100, 'crop_start': 5, 'freqs': [-1], 'n_channels': 30, 'n_roi': 8, 'bold_delay': 6, 'to_many': True, 'random_subsample': True, 'sample_per_epoch': 512, 'WINDOW_SIZE': 2048, 'optimizer': 'adamW', 'lr': 1e-05, 'weight_decay': 0.0001, 'batch_size': 16, 'mse_weight': 1.0, 'corr_weight': 0.0, 'preproc_type': 'dB_log', 'loss_function': 'corr', 'model_type': 'Best_AE_Artur_Multi_Head'}\n",
      "DeepONet(\n",
      "  (branch): BranchNet(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv1d(30, 128, kernel_size=(5,), stride=(2,), padding=(3,))\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (conv2): Conv1d(128, 64, kernel_size=(5,), stride=(2,), padding=(3,))\n",
      "    (pool): AdaptiveAvgPool1d(output_size=1)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (trunk): TrunkNet(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=16384, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=16, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([4, 30, 2048])\n",
      "torch.Size([4, 16384])\n",
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "       BranchNet-1            [4, 128]          66,912          66,912\n",
      "        TrunkNet-2            [4, 128]         264,336         264,336\n",
      "=======================================================================\n",
      "Total params: 331,248\n",
      "Trainable params: 331,248\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "Starting Training of our model \n",
      "Number of samples 512 \n",
      "Size of batch: 16 Number batches 32\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "\n",
      "Epoch 50 train loss_0 : 0.961 val loss_0 : 0.801 train loss_1 : -0.000153 val loss_1 : -0.00373 train loss_2 : 0.961 val loss_2 : 0.801 \n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "\n",
      "Epoch 100 train loss_0 : 0.963 val loss_0 : 0.801 train loss_1 : 0.00243 val loss_1 : 0.00273 train loss_2 : 0.963 val loss_2 : 0.801 \n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "\n",
      "Epoch 150 train loss_0 : 0.956 val loss_0 : 0.801 train loss_1 : 0.00289 val loss_1 : 0.00667 train loss_2 : 0.956 val loss_2 : 0.801 \n",
      "SLIDING WINDOW: True\n",
      "(8, 5120)\n",
      "(8, 5120)\n",
      "\n",
      "Epoch 200 train loss_0 : 0.968 val loss_0 : 0.801 train loss_1 : 0.0197 val loss_1 : 0.00447 train loss_2 : 0.968 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 250 train loss_0 : 0.958 val loss_0 : 0.801 train loss_1 : 0.0226 val loss_1 : -0.00435 train loss_2 : 0.958 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 300 train loss_0 : 0.953 val loss_0 : 0.801 train loss_1 : 0.0332 val loss_1 : 0.00499 train loss_2 : 0.953 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 350 train loss_0 : 0.935 val loss_0 : 0.801 train loss_1 : 0.0311 val loss_1 : -0.00317 train loss_2 : 0.935 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 400 train loss_0 : 0.951 val loss_0 : 0.801 train loss_1 : 0.0446 val loss_1 : -0.00413 train loss_2 : 0.951 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 450 train loss_0 : 0.946 val loss_0 : 0.801 train loss_1 : 0.055 val loss_1 : -0.00306 train loss_2 : 0.946 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 500 train loss_0 : 0.937 val loss_0 : 0.801 train loss_1 : 0.0644 val loss_1 : -0.00219 train loss_2 : 0.937 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 550 train loss_0 : 0.92 val loss_0 : 0.801 train loss_1 : 0.0842 val loss_1 : -0.00342 train loss_2 : 0.92 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 600 train loss_0 : 0.904 val loss_0 : 0.801 train loss_1 : 0.0892 val loss_1 : -0.00585 train loss_2 : 0.904 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 650 train loss_0 : 0.911 val loss_0 : 0.801 train loss_1 : 0.109 val loss_1 : -0.00493 train loss_2 : 0.911 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 700 train loss_0 : 0.91 val loss_0 : 0.801 train loss_1 : 0.11 val loss_1 : -0.00976 train loss_2 : 0.91 val loss_2 : 0.801 \n",
      "\n",
      "Epoch 750 train loss_0 : 0.907 val loss_0 : 0.801 train loss_1 : 0.119 val loss_1 : -0.0117 train loss_2 : 0.907 val loss_2 : 0.801 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ========== MODEL DEFINITION ==========\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "class BranchNet(nn.Module):\n",
    "    def __init__(self, input_channels, window_size, output_dim=8):  # Reduced output_dim\n",
    "        super(BranchNet, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 128, kernel_size=5, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(128, 64, kernel_size=5, stride=2, padding=3)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "\n",
    "        self.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class TrunkNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=8):  # Keep same output_dim\n",
    "        super(TrunkNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Dropout added\n",
    "            nn.Linear(16, output_dim)\n",
    "        )\n",
    "\n",
    "        self.apply(initialize_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class DeepONet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_channels, \n",
    "                 window_size, \n",
    "                 n_roi, \n",
    "                 branch_output_dim=128,   \n",
    "                 trunk_output_dim=128):    \n",
    "        super(DeepONet, self).__init__()\n",
    "        \n",
    "        self.branch = BranchNet(\n",
    "            input_channels=input_channels, \n",
    "            window_size=window_size, \n",
    "            output_dim=branch_output_dim\n",
    "        )\n",
    "        \n",
    "        self.trunk = TrunkNet(\n",
    "            input_dim=window_size * n_roi, \n",
    "            output_dim=trunk_output_dim\n",
    "        )\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.n_roi = n_roi\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        eeg, trunk_inputs = inputs\n",
    "\n",
    "        branch_out = self.branch(eeg)\n",
    "        trunk_out = self.trunk(trunk_inputs)\n",
    "\n",
    "        output = torch.einsum('bi,bj->bij', branch_out, trunk_out)\n",
    "        output = output.view(-1, self.n_roi, self.window_size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_step(x_batch, y_batch, model, optimizer, loss_function):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Gradient clipping\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    outputs = model(x_batch)\n",
    "    losses = loss_function(outputs, y_batch)\n",
    "    \n",
    "    #losses = [loss + 1e-8 if torch.any(loss == 0) else loss for loss in losses]\n",
    "    #print(losses)\n",
    "\n",
    "    losses[0].backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return losses\n",
    "\n",
    "n_runs = 1\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # Model initialization\n",
    "    model = DeepONet(\n",
    "        input_channels=config['n_channels'], \n",
    "        window_size=config['WINDOW_SIZE'], \n",
    "        n_roi=config['n_roi']\n",
    "    )\n",
    "\n",
    "    loss_func = train_utils.make_new_complex_loss_function(mse_weight = config['mse_weight'], \n",
    "                                                       corr_weight = config['corr_weight'])\n",
    "    \n",
    "    train_step = train_step\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=config['lr'], \n",
    "                       weight_decay=config['weight_decay'])\n",
    "    \n",
    "    \n",
    "    parameters = {\n",
    "        'EPOCHS': 4500,\n",
    "        'model': model, \n",
    "        'train_loader': train_loader, \n",
    "        'val_loader': val_loader, \n",
    "        'loss_function': loss_func,\n",
    "        'train_step': train_step,\n",
    "        'optimizer': optimizer, \n",
    "        'device': 'cuda', \n",
    "        'raw_test_data': test_dataset_prep,\n",
    "        'show_info': 50, \n",
    "        'num_losses': 5,\n",
    "        'labels': labels_roi,\n",
    "        'inference_function': inference.new_model_inference_function, \n",
    "        'to_many': config['to_many']\n",
    "    }\n",
    "\n",
    "    path_to_save_wandb = '/home/lyz6/Documents'\n",
    "\n",
    "    with wandb.init(project=\"eeg_fmri\", config=config, save_code=True):\n",
    "        wandb.define_metric(\"val/corr_mean\", summary=\"max\")\n",
    "\n",
    "        if i == 0: \n",
    "            exp_name = wandb.run.name\n",
    "        \n",
    "        wandb.run.name = exp_name +'_run_' + str(i)\n",
    "        \n",
    "        print(config)\n",
    "        print(parameters['model'])\n",
    "        \n",
    "        # Create dummy inputs for the model: EEG signals and time indices\n",
    "        dummy_eeg = torch.zeros(4, config['n_channels'], config['WINDOW_SIZE'])  # Batch of 4 EEG samples\n",
    "        dummy_time = torch.zeros(4, config['WINDOW_SIZE'] * 8)  # Batch of 4 normalized time indices\n",
    "        print(dummy_eeg.shape)\n",
    "        print(dummy_time.shape)\n",
    "        \n",
    "        # Call the summary function with both inputs\n",
    "        print(summary(model, (dummy_eeg, dummy_time), show_input=False))\n",
    "\n",
    "        model = train_utils.new_train_regression(**parameters)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1485254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "class DeepONet(nn.Module):\n",
    "    def __init__(self, n_channels, window_size, n_roi, latent_dim=16):\n",
    "        super(DeepONet, self).__init__()\n",
    "        self.branch = BranchNet(input_channels=n_channels, output_dim=latent_dim)\n",
    "        self.trunk = TrunkNet(input_dim=window_size, output_dim=latent_dim)\n",
    "        self.n_roi = n_roi  # Number of regions of interest (output dimension)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        eeg, trunk_inputs = inputs\n",
    "        batch_size, time_steps = trunk_inputs.size(0), trunk_inputs.size(1)\n",
    "\n",
    "        # Outputs from BranchNet and TrunkNet\n",
    "        branch_out = self.branch(eeg)  # [batch_size, latent_dim]\n",
    "        trunk_out = self.trunk(trunk_inputs)  # [batch_size, time_steps, latent_dim]\n",
    "\n",
    "        # Compute the inner product across the latent_dim\n",
    "        branch_out = branch_out.unsqueeze(1)  # [batch_size, 1, latent_dim]\n",
    "        output = torch.sum(branch_out * trunk_out, dim=-1)  # [batch_size, time_steps]\n",
    "\n",
    "        # Add extra dimension for ROI-specific outputs\n",
    "        output = output.unsqueeze(1).expand(-1, self.n_roi, -1)  # [batch_size, n_roi, time_steps]\n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "class DeepONet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_channels, \n",
    "                 window_size, \n",
    "                 n_roi,\n",
    "                 branch_out_dim=16, \n",
    "                 trunk_out_dim=8):\n",
    "        super(DeepONet, self).__init__()\n",
    "        self.branch = BranchNet(input_channels=n_channels, output_dim=branch_out_dim)\n",
    "        \n",
    "        self.trunk = TrunkNet(\n",
    "            input_dim=window_size,  # Entire time series as input\n",
    "            output_dim=trunk_out_dim\n",
    "        )\n",
    "        \n",
    "        # Prediction layer\n",
    "        self.fc = nn.Linear(branch_out_dim + trunk_out_dim, n_roi)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        eeg, trunk_inputs = inputs  # Unpack the inputs tuple\n",
    "        branch_out = self.branch(eeg)  # [batch_size, branch_out_dim]\n",
    "        trunk_out = self.trunk(trunk_inputs)  # [batch_size, trunk_out_dim]\n",
    "        combined = torch.cat([branch_out, trunk_out], dim=-1)\n",
    "        return self.fc(combined)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e40a09-12c2-43b9-957f-8b37d0a0daa8",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
